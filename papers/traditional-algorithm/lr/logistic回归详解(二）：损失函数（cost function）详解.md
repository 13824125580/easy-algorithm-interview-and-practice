## 1.有监督学习
机器学习分为有监督学习，无监督学习，半监督学习，强化学习。对于逻辑回归来说，就是一种典型的有监督学习。  
既然是有监督学习，训练集自然可以用如下方式表述：  
$$\{(x^1,y^1),(x^2,y^2),\cdots,(x^m,y^m)\}$$  

对于这m个训练样本，每个样本本身有n维特征。再加上一个偏置项$x_0$, 则每个样本包含n+1维特征：  
$$x = [x_0,x_1,x_2,\cdots,x_n]^T$$  
其中 $x\in R^{n+1}$, $x_0=1$, $y\in\{0,1\}$  

李航博士在统计学习方法一书中给分类问题做了如下定义：  
分类是监督学习的一个核心问题，在监督学习中，当输出变量Y取有限个离散值时，预测问题便成为分类问题。这时，输入变量X可以是离散的，也可以是连续的。监督学习从数据中学习一个分类模型或分类决策函数，称为分类器(classifier)。分类器对新的输入进行输出的预测(prediction)，称为分类(classification).  

在logistic回归详解一(http://blog.csdn.net/bitcarmanlee/article/details/51154481)  
中，我们花了一整篇篇幅阐述了为什么要使用logistic函数:   
$$h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}$$  
其中一个重要的原因，就是要将Hypothesis(NG课程里的说法)的输出映射到0与1之间，既：    
$$0\le h_{\theta}(x)\le 1$$  

同样是李航博士统计学习方法一书中，有以下描述：  
统计学习方法都是由模型，策略，和算法构成的，即统计学习方法由三要素构成，可以简单表示为：  
$$方法 = 模型 + 策略 + 算法$$  

对于logistic回归来说，模型自然就是logistic回归，策略最常用的方法是用一个损失函数(loss function)或代价函数(cost function)来度量预测错误程度，算法则是求解过程，后期会详细描述相关的优化算法。  

## 2.logistic函数求导
$$\begin{aligned}
g'(z) & = \frac{d}{dz}\frac{1}{1+e^{-z}} \\\\
& = \frac{1}{(1+e^{-z})^2} (e^{-z}) \\\\
& = \frac{1}{(1+e^{-z})} \cdot \left (1 - \frac{1}{(1+e^{-z})} \right) \\\\
& = g(z)(1-g(z))
\end{aligned}$$  

此求导公式在后续推导中会使用到  

## 3.常见的损失函数
机器学习或者统计机器学习常见的损失函数如下：  

1.0-1损失函数 （0-1 loss function）  
$$ L(Y,f(X))= \begin{cases} 1 , & \text {Y $\neq$ f(X)} \\ 0, & \text{Y = f(X)} \end{cases} $$  

2.平方损失函数（quadratic loss function)  
$$L(Y,f(X)) = (Y - f(x))^2$$  

3.绝对值损失函数(absolute loss function)
$$L(Y,f(x)) = |Y - f(X)|$$

4.对数损失函数（logarithmic loss function) 或对数似然损失函数(log-likehood loss function)  
$$L(Y,P(Y|X)) = -logP(Y|X)$$  

逻辑回归中，采用的则是对数损失函数。如果损失函数越小，表示模型越好。  

## 4.说说对数损失函数与平方损失函数
在逻辑回归的推导中国，我们假设样本是服从伯努利分布(0-1分布)的，然后求得满足该分布的似然函数，最终求该似然函数的极大值。整体的思想就是求极大似然函数的思想。而取对数，只是为了方便我们的在求MLE(Maximum Likelihood Estimation)过程中采取的一种数学手段而已。  

## 损失函数详解
根据上面的内容，我们可以得到逻辑回归的对数似然损失函数cost function：  
$$cost(h_{\theta}(x),y) = \begin{cases}  -log(h_{\theta}(x))  & \text {if y=1} \\\\ -log(1-h_{\theta}(x))  & \text{if y=0} \end{cases} $$    

稍微解释下这个损失函数，或者说解释下对数似然损失函数：    
当y=1时，假定这个样本为正类。如果此时$h_\theta(x)=1$,则单对这个样本而言的cost=0,表示这个样本的预测完全准确。那如果所有样本都预测准确，总的cost=0  

但是如果此时预测的概率$h_\theta(x)=0$，那么$cost\to\infty$。直观解释的话，由于此时样本为一个正样本，但是预测的结果$P(y=1|x;\theta) = 0$, 也就是说预测 y=1的概率为0，那么此时就要对损失函数加一个很大的惩罚项。    

当y=0时，推理过程跟上述完全一致，不再累赘。    

将以上两个表达式合并为一个，则单个样本的损失函数可以描述为：  

$$cost(h_{\theta}(x),y) = -y_ilog(h_{\theta}(x)) - (1-y_i)log(1-h_{\theta}(x))$$  
因为 $y_i$ 只有两种取值情况，1或0，分别令y=1或y=0，即可得到原来的分段表示式。  

全体样本的损失函数可以表示为：  
$$cost(h_{\theta}(x),y) = \sum_{i=1}^{m} -y_ilog(h_{\theta}(x)) - (1-y_i)log(1-h_{\theta}(x))$$  
这就是逻辑回归最终的损失函数表达式  