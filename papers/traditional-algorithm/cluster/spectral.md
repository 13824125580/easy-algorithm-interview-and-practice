## 1.谱聚类概述
谱聚类(Spectral clustering)是利用矩阵的特征向量进行聚类的一种方法，其本质上是矩阵特征分解进行降维的一种方法。它一般由两部分组成，第一部分是对数据进行变换，第二部分再使用传统的kmeans等方法对变换以后的数据进行聚类。  
谱聚类是从图论中演化出来的算法，后来在聚类中得到了广泛的应用。它的主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重和尽可能的低，而子图内的边权重和尽可能的高，从而达到聚类的目的。(这一段内容来自刘建平老师的描述，具体见参考文献1)  

## 2.谱聚类的基本步骤
在具体的内容展开之前，可以看一下谱聚类的基本步骤  
1.先确定参数，包括一个描述相似性图的邻接矩阵W，与聚类的目标类书目k。  
2.计算度(次数)矩阵D与拉普拉斯矩阵L。  
3.计算标准化以后的拉普拉斯矩阵$D^{-1/2}LD^{1/2}$。  
4.求$D^{-1/2}LD^{1/2}$最小k_1个特征值所对应的的特征向量$f_k$。  
5.将上述的特征向量$f_k$构成的矩阵行标准化，得到$n*k$矩阵。  
6.对上述的n个样本用诸如kmeans等方法进行聚类，聚类维数为$k_2$。  
7.得到簇的划分$C(c_1, c_2, \cdots, c_{k2})$  

## 3.图的基本概念
图是数据结构中的概念。图是由顶点与边构成，任意两个节点之间可能都有边进行连接，如果边带有值的信息，称为权重。比如下面的一个图。  

![在这里插入图片描述](https://github.com/bitcarmanlee/easy-algorithm-interview-photo/blob/master/traditional-algorithm/cluster/spectral/1.png)  
图一般用字母G表示，边一般用E表示，点用V表示。如果两个点之间有连接，则有一条边。如果边是单向的，则边是有向的，该图为有向图。否则为无向图。  

顶点的度定义为该顶点所关联的边的数量，对于有向图它还分为出度和入度，出度是指从一个顶点射出的边的数量，入度是连入一个节点的边的数量。无向图可以用三元组形式化的表示：  
$$(V, E, \omega)$$  

其中，V是顶点集合，E是边的集合，$\omega$是边的权重。假设i和j为图的顶点，$W_{ij}$为边(i,j)的权重，由它构成的矩阵W称为邻接矩阵。那么对于无向图来说，其邻接矩阵为对称矩阵。  

对于图中的任意一个点$v_i$，定义他的度$d_i$为和他连接的所有边的权重之和  
$$d_i = \sum \limits_{j=1}^{n}w_{ij}$$  

利用每个点度的定义，可以得到一个n*n的度矩阵D，其为对角矩阵，只有局对角线有值，对应第i个点的度。  
$$
 D = \left[
 \begin{matrix}
   d_1 & \cdots & \cdots \\\\
   \cdots & d_2 & \cdots \\\\
   \cdots & \cdots & \cdots \\\\
   \cdots & \cdots & d_n \\\\
  \end{matrix}
  \right] \tag{3}
$$

## 4.相似矩阵与拉普拉斯矩阵
上面提到了邻接矩阵，那么怎样得到邻接矩阵？常见的构建方法是对第i个数据点，离该点最近的p个数据点使用它们之间的欧几里得距离作为相似性的度量，其他较远的点相似性程度都为0。一旦构建好了相似性图G，我们就可以写下用来描述这个图的临接矩阵W。  
一般由三种方式构建邻接矩阵，$\epsilon$邻近，K邻近与全链接。  

$\epsilon$邻近是设置一个阈值$\epsilon$，然后计算任意两点ij之间的距离$s_{}ij = || x_i - x_j || \_2 ^ 2$  。    
$$P(x|Pa_x)=\begin{cases} 
		0, & s_{ij} > \epsilon\\\\ 
		\epsilon, & s_{ij} \leq \epsilon 
\end{cases}$$  

第二种方式是K邻近法，找到每个样本最近的k个点作为近邻，只有和样本距离最近的k个点之间的$\omega_{ij} > 0$  
全连接法相比较前两种，所有的点之间的权重值都大于0，因此称之为全连接法。可以选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和Sigmoid核函数。最常用的是高斯核函数RBF，此时相似矩阵和邻接矩阵相同：  

$$w_{ij} = s_{ij} = exp(- \frac{||x_i - x_j||_2^2}{2\sigma ^ 2})$$  
其中，$\sigma$控制邻域的宽度，是人工设置的超参数。  

然后我们可以定义拉普拉斯矩阵L = D - W，其中D为上面讲到的度矩阵，是一个对角矩阵。W为邻接矩阵，可以由上面提到的方法构造出来。因为D是对角矩阵，而W为对称矩阵，很明显L也为对称矩阵。  

## 5.图的切割
对于无向图G，我们的目标是将G(V, E)切成相互没有连接的k个子图，每个子图点的集合为：  
$A_1, A_2, \cdots, A_k$。  
对于任意两个子图点的集合A, B，定义AB之间的切图权重为  
$$W(A, B) = \sum \limits \_{i \in A, j \in B} w_{ij}$$    
那么对于k个子图的集合$A_1, A_2, \cdots, A_k$，定义图的cut  
$$cut(A_1, A_2, \cdots, A_k) =  \frac{1}{2}\sum \limits _{i=1}^k W(A_i, \overline {A_i})$$  
那么切图的要求自然就是高内聚，低耦合了，就是说每个子图内的点权重高，而每个子图之间的点权重和低。一个自然的想法就是最小化$cut(A_1, A_2, \cdots, A_k)$，但是这样会存在问题。  
![在这里插入图片描述](https://github.com/bitcarmanlee/easy-algorithm-interview-photo/blob/master/traditional-algorithm/cluster/spectral/2.jpg)  
上图可以看出smallest cut跟best cut差别很大。  

## 6.不同的切图方式
### 6.1 RatioCut 
为了避免上面的最小切图，除了考虑$cut(A_1, A_2, \cdots, A_k)$，我们还考虑最大化每个子图点的个数，即  

$$Ratiocut(A_1, A_2, \cdots, A_k) = \frac{1}{2} \sum \limits _{i=1}^k \frac{W(A_i, \overline {A_i})}{|A_i|}$$  

### 6.2 Ncut  
Ncut与RatioCut不一样的地方在于，将分母$|A_i|$换成了$vol(A_i)$。其中，我们将$vol(A_i)$定义为：  
$$vol(A_i) = \sum \limits _{i \in A} d_i$$  


## 7.谱聚类的优缺点
优点：  
1.算法过程中只用到数据点之间的相似矩阵，所以处理稀疏数据毫无压力。  
2.因为在矩阵分解的过程中，会有降维，因此处理高维数据的时候复杂度比kmeans等算法低。  
缺点：  
1.最终的效果依赖相似矩阵，不同的相似矩阵结果差别比较大。  
2.如果聚类的维度很高，可能因为降维幅度不够，谱聚类的运行速度和最后的聚类效果均不好。  

## 8.总结
综上所述，谱聚类是一种基于图的算法。先把样本数据看成图的顶点，根据数据点之间的距离构造边，形成带权重的图，然后通过对图进行处理来完成算法所需的功能。对于聚类问题，通过图的切割实现聚类，即将图切分成多个子图，这些子图就是对应的簇。  



#  参考文献
1.https://www.cnblogs.com/pinard/p/6221564.html