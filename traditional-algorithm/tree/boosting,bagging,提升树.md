## 0.集成学习(ensemble learning)
集成学习是目前一种比较流行的学习算法，是一大类学习算法的统称，而不是单独的某个算法。其核心思想是三个臭皮匠，顶个诸葛亮。某一个学习器的能力可能比较弱，但是将多个学习器集合在一起，发挥集体的智慧，可能就能迸发出强大的战斗力。

集成学习主要有两个问题需要解决：第一是如何得到若干个弱学习器，第二则是采取一种什么样的策略让这些弱学习器组合到一起。
对于第一个问题，我们目前采取的方式一般都是将同一种弱学习器训练多个，比如常见的GBDT，采用的弱学习器就是决策树这种结构。而第二个问题，目前一般由两种方式，boosting与bagging。下面先介绍一下boosting与bagging的区别。

## 1.Boost VS bagging

boost一般的算法流程是：
1.通过加法模型加弱分类器进行线性组合。
2.每轮训练过程，给错误率较小的弱分类器加大权重，而对错误率较大的模型降低权重。
3.每轮改变训练数据的权重或概率分布，通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

bagging一般的算法流程是：
1.从原始数据中抽样获取训练集，抽样是有放回的，有些样本可以被多次抽取到，有些样本可能一次都没有被抽中。共进行k轮抽取，得到k个训练集。
2.每次使用一个训练集得到一个弱学习器，一共得到k个弱学习器。
3.如果是分类问题，将k个弱学习器投票。如果是回归问题，可以将k个学习器的结果取均值作为最终结果。

从上面boost与bagging的流程不难看出，boosting更多关注的未正确分类的样本，追求的是准确性，所以bias(偏差)比较小。而bagging追求的是公平，给每个样本同等的概率，所以virance(方差)比较小。

boosting与bagging的区别在于
1.选择样本
bagging是独立的有放回抽样，抽样过程中选取k个训练集是独立的。
而boost每轮训练的训练集不变，只是每个样本的权重发生变化。而权重是根据上轮分类结果来进行调整。
2.样本权重
bagging是均匀的有放回抽样，每个样本权重相等。
boosting是根据分类结果取权重，错误率越高样本权重越大。
3.弱分类器权重
bagging中所有弱分类器权重相等。
boosting中准确率越高的分类器权重越大。
4.并行
bagging的弱分类器是可以并行训练的
boosting因为分类器有依赖关系，下一轮训练依赖上一轮的结果，因此不可并行。

## 2.Boosting Decision Tree(提升树)
从名字就很容易看出来，提升树属于boosting系列算法，其弱分类器是决策树(Decision Tree)。

提升树可以用加法模型来表示
$$f_M(x) = \sum_{m=1}^M T(x; \theta_m)$$
其中，$T(x; \theta_m)$表示一棵决策树，$\theta_m$为树的参数，$M$为树的棵数。

具体的算法流程为：
假设X与Y分别为输入输出，Y是连续变量。$$D = \{ (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)) \}$$
一、初始化$f_0(x) = 0$
二、对m=1, 2, ...M
2.1 计算残差 $r_{mi} = y_i - f_{m-1}(x_i), i=1, 2,..n$
2.2 拟合残差，得到新的回归树 $T(x; \theta_m)$
2.3 更新$f_m(x) = f_{m-1}(x) + T(x; \theta_m)$
三、得到最终模型
$$f_M(x) = \sum_{m=1}^M T(x; \theta_m)$$

具体$T(x; \theta_m)$怎么训练得到，可以参考[CART回归树](https://blog.csdn.net/bitcarmanlee/article/details/106824993)一文。

## 3.例子
具体提升树的例子，可以参考https://zhuanlan.zhihu.com/p/35796662一文中提到的统计学习方法一书中的例子。