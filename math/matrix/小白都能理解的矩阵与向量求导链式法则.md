## 0.前言
深度学习中最常见的是各种向量还有矩阵运算，经常会涉及到求导操作。因此准确理解向量矩阵的求导操作就显得非常重要，对我们推导计算过程以及代码书写核对有非常大的帮助。  
神经网络中最常见的操作为向量，矩阵乘法，在求导的时候经常需要用到链式法则，链式法则在计算过程中会稍微麻烦，下面我们来详细推导一下，推导过程全程简单明了，稍微有点数学基础的同学都能看明白。  

## 1.标量对标量的链式求导
假设x, y, z都为标量(或者说一维向量)，链式关系为x -> y -> z。根据高数中的链式法则  
$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}$$  

上面的计算过程很简单，不多解释。  

## 2.向量对向量链式求导  
假设x,y,z都为向量，链式关系为x -> y -> z。如果我们要求$\frac{\partial z}{\partial x}$，可以直接用链接法则求导  
$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}$$  

假设x, y, z的维度分别为m, n, p，$\frac{\partial z}{\partial x}$的维度为p * m，而$\frac{\partial z}{\partial y}$的维度为p * n，$\frac{\partial y}{\partial x}$的维度为n * m，p * n与n * m的维度刚好为p * m，与左边相同。  

## 3.标量对多向量的链式求导
在深度学习中，一般我们的损失函数为一个标量函数，比如MSE或者Cross Entropy，因此最后求导的目标函数为标量。  
假设我们最终优化的目标为z是个标量，x,y分为是m,n维向量，依赖关系为x->y->z。现在需要求的是$\frac{\partial z}{\partial x}$，维度为m * 1。  
易知有$\frac{\partial z}{\partial y}$为n * 1， $\frac{\partial y}{\partial x}$为n * m，则$(\frac{\partial y}{\partial x}) ^ T \cdot \frac{\partial z}{\partial y}$的维度为m * 1，与左边能对上。  
因此有  
$$\frac{\partial z}{\partial x} = (\frac{\partial y}{\partial x}) ^ T \cdot \frac{\partial z}{\partial y}$$  
扩展到多个向量  
y1 -> y2 -> y3 -> ...-> yn -> z  
$$ \frac{\partial z}{\partial y_1} = (\frac{\partial y_n}{\partial y_{n-1}} \cdot \frac{\partial y_{n-1}}{\partial y_{n-2}} \cdots \frac{\partial y_2}{\partial y_1}) ^ T \cdot \frac{\partial z}{\partial y_n}$$  

以常见的最小二乘求导为例：  
$$C = (X\theta - y) ^ T (X\theta - y)$$  
损失函数C是个标量，假设X为m\*n的矩阵，$\theta$为$n*1$的向量，我们要求C对$\theta$的导数，令$z = X\theta - y$，$C = z^Tz$，由上面的链式关系  
$$\frac{\partial C}{\partial \theta} =  (\frac{\partial z}{\partial \theta})^T \cdot \frac{\partial C}{\partial z} = 2X^T(X \theta - y)$$  

核对一下维度  
$\frac{\partial C}{\partial \theta}$是n * 1, $X^T$是n * m，$X \theta - y$是m * 1， $X^T (X \theta - y)$是n * 1，能与左边对上。  

其中  
$$\frac{\partial (X \theta - y)}{\partial \theta} = X$$  
$$\frac{\partial (z^Tz)}{\partial z} = 2z$$  

$X \theta - y$为m * 1, $\theta$为n * 1， X的维度刚好为m * n。  
$z^Tz$是个标量，对$z$求导结果为$2z$，与$z$的维度一致。  

所以最小二乘最优解的矩阵表达式为  
$$2X^T(X \theta - y) = 0$$  
$$\theta = (X^TX)^{-1}X^Ty$$  

## 4.标量对多矩阵链式求导
神经网络中，最常见的计算方式是$Y = WX + b$，其中$W$为权值矩阵。  
看个更为常规的描述：  
假设$z = f(Y)$，$Y = AX + B$，其中A为m * k矩阵，X为k * 1向量，B为m * 1向量，那么Y也为m * 1向量，z为一个标量    

如果要求$\frac{\partial z}{\partial X}$，结果为k * 1的维度。$\frac{\partial z}{\partial Y}$的维度为m * 1， A的维度为m * k  
$$\frac{\partial z}{\partial X} =  A ^ T \cdot \frac{\partial z}{\partial Y}$$  

左边的维度为k * 1，右边的维度为k * m 与m * 1相乘，也是k * 1，刚好能对上。  

当X为矩阵时，按同样的方式进行推导可以得到一样的结论。  

如果要求$\frac{\partial z}{\partial A}$，结果为m * k矩阵。$\frac{\partial z}{\partial Y}$的维度为m * 1， X的维度为k * 1，  
$$\frac{\partial z}{\partial A} =  \frac{\partial z}{\partial Y} \cdot X^T$$  
当X为矩阵时，按同样的方式进行推导可以得到一样的结论。  


## 5.总结
一句话总结就是：标量对向量或者矩阵进行链式求导的时候，按照维度将结果对其就特别容易推导。  